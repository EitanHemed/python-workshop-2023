{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Python-Workshop-Day-2.ipynb",
   "provenance": [
    {
     "file_id": "1LiCq5EPFYtlJFYA2_qFnKPAp9OhbYN6e",
     "timestamp": 1633772348264
    }
   ],
   "collapsed_sections": [],
   "toc_visible": true,
   "authorship_tag": "ABX9TyNJ7UsCRcm+TxKhhwFj+g4w"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HTyANlELsPz_"
   },
   "source": [
    "## Pandas\n",
    "\n",
    "pandas ('panel-data') is the main library for working with tabular data in Python on small data sets (as a rule of thumb, less than 1GB). Before learning how to read and write CSV/Excel files, we will go over the basics of pandas.\n",
    "\n",
    "----\n",
    "\n",
    "The main object you will work with in Pandas is a dataframe (`pd.DataFrame`).\n",
    "A dataframe is a table, but it offers much more than just a matrix of values. \n",
    "\n",
    "![Anatomy of a dataframe](https://media.geeksforgeeks.org/wp-content/cdn-uploads/creating_dataframe1.png) \n",
    "\n",
    "([source](https://media.geeksforgeeks.org/wp-content/cdn-uploads/creating_dataframe1.png))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cBsrOQaQHnt5"
   },
   "source": [
    "### Series\n",
    "\n",
    "A dataframe is composed of columns, each series is 1-D nd-array, with axis labels. We can create a series from a list of an array of values."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kzVW26aXH5Cm"
   },
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "student_grades = pd.Series(\n",
    "    data=np.random.normal(90, 1.5, size=5), \n",
    "    index=list('ABCDE'), name='student_grades'\n",
    ")\n",
    "print(student_grades, \n",
    "    student_grades.shape,\n",
    "    student_grades.values,\n",
    "    student_grades.name, sep='\\n\\n')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lyKkobdQLrLo"
   },
   "source": [
    "### DataFrame\n",
    "\n",
    "A data frame is a collection of series objects, known as columns. Dataframes are potentially hetrogenous, unlike arrays, as each column can have its own data type.\n",
    "\n",
    "----\n",
    "\n",
    "We will now create a dataframe but we will not give it any special column names (label-based identifier for columns - axis 1) or row names (index - label based\n",
    "identifier for rows - axis 0). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4MDZ7i_cOCbA"
   },
   "source": [
    "import string\n",
    "\n",
    "# Create 5 columns of 20 values each, sampled from a random distribution\n",
    "random_numbers = pd.DataFrame(\n",
    "    data=np.random.normal(size=(20, 5)))\n",
    "\n",
    "# Add a 6th column that contains random strings\n",
    "random_numbers[5] = np.random.choice(['dog', 'cat', 'bear', 'bird'],\n",
    "                                     size=random_numbers.shape[0])\n",
    "\n",
    "print(random_numbers,\n",
    "      random_numbers.columns,\n",
    "      random_numbers.index, sep='\\n\\n')\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ezWT_Hw5RQdD"
   },
   "source": [
    "We can get some information on our dataframe using `df.info` e.g., the numebr of null values on each column, their names and data types."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yLq83TbHRNRO"
   },
   "source": [
    "random_numbers.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKBsUZ7vOw-w"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "Refering to columns using an integer index doesn't add much over arrays. This is way we can use column names.\n",
    "\n",
    "Change the name of the dataframe columns using multiple ways. Print the new column names after each change to see what happened.\n",
    "*   First using assignment on creation.\n",
    "*   Second, update the column names by using the `pd.DataFrame.rename` method (e.g., change names, capitalization, etc.).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "T2pPGibqOwiS"
   },
   "source": [
    "course_grades = pd.DataFrame(\n",
    "    data=[\n",
    "          # Specifying each column values for each row\n",
    "          ('Python 101', 'Fall', 95, 2021), \n",
    "          ('Python 101', 'Spring', 85, 2020), \n",
    "          ('Python 101', 'Fall', 90, 2019), \n",
    "          ('Python 102', 'Fall', 95, 2021), \n",
    "          ('Python 102', 'Summer', 100, 2020), \n",
    "          ('Python 102', 'Fall', 90, 2019), \n",
    "    ], columns=['Course', 'Term', 'Average', 'Year'])\n",
    "\n",
    "print(course_grades)\n",
    "\n",
    "course_grades = course_grades.rename(\n",
    "    dict(zip(course_grades.columns, \n",
    "             ['Course Name', 'Semester', 'Mean Grade', 'Date'])), \n",
    "    axis=1)\n",
    "\n",
    "print(course_grades) # Why has it not changed?\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kQauZG-g03qZ"
   },
   "source": [
    "The crux of the previous exercise was that in Pandas, you have to be aware what are the effects of your actions. Many of the functions return a copy of the dataframe with the additional change from the function call, rather than change it in place by default. Using `inplace=True` is a matter of choice, but there are [debates](https://github.com/pandas-dev/pandas/issues/16529) for and against it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IP43k_wc51Nn"
   },
   "source": [
    "### Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aAtLqsy6_Z9g"
   },
   "source": [
    "In pandas you can select columns, rows or both in multiple ways. To demonstrate and practice it we will load an example dataset from a library that we'll get to know later."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hyx3nZY35htv"
   },
   "source": [
    "import seaborn as sns\n",
    "\n",
    "# mpg (miles per gallon) is a good data set for this section as it contains \n",
    "#  both numeric and string columns\n",
    "\n",
    "mpg = sns.load_dataset('mpg')\n",
    "\n",
    "print(mpg.info())\n",
    "\n",
    "mpg.head() # prints the first ten rows, to get a feel of the data"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sPH4xjrVB_5j"
   },
   "source": [
    "We can use square brackets to retrieve a single column (i.e., a series)"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oYN2Hl6T4mjW"
   },
   "source": [
    "mpg['origin']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NfjdEsvyCZCk"
   },
   "source": [
    "You can also use mpg.origin to get the same result, although alluring for newcomers, this is not recommended. \n",
    "* You cannot retrieve a column this way if it has spaces in it (`df.total price`).\n",
    "* You cannot store the column name in another variable. (`x = 'col_name'; df.x`)\n",
    "* You cannot retrieve a couple of columns together.\n",
    "\n",
    "**Use square brackets syntax.**"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZZVyBAMsC5iI"
   },
   "source": [
    "mpg[['model_year', 'weight']] # You can retrieve multiple columns in a new order"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sW5XSmPoEF94"
   },
   "source": [
    "An important thing to note is that the returned values, are a *view* or a slice \n",
    "of the dataframe and not a new object. This means that changes that are applied\n",
    "to it will be reflected in the original dataframe as well. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "z2nIh_ZXEacu"
   },
   "source": [
    "mpg['model_year'] += 1900 # This is why reassignment works here. \n",
    "mpg.head() # The original values are greater by 1900"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEeccYcjFjF-"
   },
   "source": [
    "There are four main ways to select rows and columns based on an index. We will cover only two of them. \n",
    "\n",
    "----\n",
    "\n",
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "`iloc` stands for integer-location. We know that a dataframe is in some sense a collection of NumPy arrays, and we know how to index 2-D arrays. So we know how to use iloc. \n",
    "\n",
    "Fill the code below to select every third row (axis=0) and every second column (axis=1) in the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ey7rlYxkF-7j"
   },
   "source": [
    "mpg.iloc[2::3, 1::2]\n",
    "mpg.iloc[0, 1] # This returns the first row, second column\n",
    "\n",
    "# Booleans also work (clunky for demo purposes).\n",
    "#  6th row and below, mid-three columns\n",
    "mpg.iloc[5:, [False, False, False, True, True, True, False, False, False]] "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AJRf8KMLEY4v"
   },
   "source": [
    "[iloc is flexible, but can only be fed integers. For more info take a look at the [documentation](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.iloc.html).\n",
    "\n",
    "----\n",
    "\n",
    "The next method of selection is `loc`, selection by label (also by boolean indexing). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "TzhdUwJTJ4-g"
   },
   "source": [
    "mpg.loc[:, ['mpg', 'model_year']] "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZeduY-T7LjHi"
   },
   "source": [
    "mpg.set_index(['name', 'origin'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N7_Q-vNkKmxR"
   },
   "source": [
    "Up until now, we didn't make use of the dataframe row-index (the row labels). They were always integer so we could just select them as on iloc, if required, the index can be turned into a string for example."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1lMMpEZNNYAP"
   },
   "source": [
    "mpg.set_index(['name']).loc[('toyota corolla')]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K_sZhp5RNbaR"
   },
   "source": [
    "However, a much cleaner way to put this in the current case (and most) would be to just use `loc` with boolean indexing."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fDU01_M1Nk6-"
   },
   "source": [
    "# mpg['name'] == 'toyota corolla' # rutrns a series of booleans\n",
    "mpg.loc[mpg['name'] == 'toyota corolla']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EhO_jN2sOCfs"
   },
   "source": [
    "Which is very specific and flexible, but could require slightly different syntax than what we know (see below).\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nE-YUenQOBj1"
   },
   "source": [
    "mpg.loc[\n",
    "        # Select non-japanese models\n",
    "        (mpg['origin'] != 'japan') \n",
    "        # Models from 1976 or later\n",
    "        & ~(mpg['model_year'] > 1975) \n",
    "        # Find if the model name name contains 'volvo or ford'\n",
    "        & (mpg['name'].str.contains('volvo|ford')) \n",
    "    ] "
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E21Wm3gtPELn"
   },
   "source": [
    "You've noticed that we didn't use the regular `and` and `not` keywords when chaining arguments. Here you are required to use bitwise-operators.\n",
    "\n",
    "The short version is:\n",
    "* When chaining conditions use paranthesis. \n",
    "* Instead of `and` use `&`\n",
    "* Instead of `or` use `|`\n",
    "* Instead of `not` use `~`\n",
    "\n",
    "If you want the long version, go [here](https://towardsdatascience.com/bitwise-operators-and-chaining-comparisons-in-pandas-d3a559487525). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7rSk5iRRq0Td"
   },
   "source": [
    "### Setting and mutating\n",
    "\n",
    "There are multiple ways by which you can update existing values in the dataframe or add new. \n",
    "\n",
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "Setting with enlargement is a method in which we \"try\" to index inexistent indices and set their values. Create a new column called 'kpg' (kilometers per gallon; mpg multiplied by 1.609)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6qKS8G0pqz3m"
   },
   "source": [
    "mpg['kpg'] = mpg['mpg'] * 1.609\n",
    "mpg.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ko2dDbzLtRLb"
   },
   "source": [
    "The same goes for adding new rows. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "h4qF6zIQtW7f"
   },
   "source": [
    "# We are using some null values for Lada, as we don't have the mpg data\n",
    "mpg.loc[mpg.shape[0]] = (\n",
    "    np.nan, 4, 95.69, 78, 2535.32, 23, 1977, 'soviet union', 'Lada Niva', np.nan)\n",
    "mpg.tail()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBUheOGn0OFy"
   },
   "source": [
    "### GroupBy\n",
    "\n",
    "\"Group by\" is a way to do one or more of the following steps: \n",
    "* Split the dataframe into groups.\n",
    "* Apply a function to each group (e.g., calculate summary statistics).\n",
    "* Recombine the results into a dataframe.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ZE5fLeXw4rgb"
   },
   "source": [
    "mpg.groupby(['origin', 'cylinders']).mean().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MYdynjYQ5Hv7"
   },
   "source": [
    "Let's break it down.\n",
    "\n",
    "`groupby` takes column name(s) as the variable that will contain our identifiers, the names of each group. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4v25gcQY5YRQ"
   },
   "source": [
    "gb = mpg.groupby('origin')\n",
    "gb.groups['japan'] # Returns the indices from the original dataframe"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "slPkPagX5qFB"
   },
   "source": [
    "We can grab a specific group from the groupby object:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GsQziICd5pHT"
   },
   "source": [
    "usa = gb.get_group('usa')\n",
    "usa.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1tCEJkkq5559"
   },
   "source": [
    "We can apply all sorts of transformations or aggregations on the group."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "GESHrSBd6Bgp"
   },
   "source": [
    "usa.agg('mean') # usa.agg(my_unique_agg_function)\n",
    "usa.mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FS1wy0Rx6A6x"
   },
   "source": [
    "And we can iterate over groups, which is a common matplotlib-pandas idiom."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xB-OIssR6tXz"
   },
   "source": [
    "import matplotlib.pyplot as plt\n",
    "fig, axs = plt.subplots(2, 2, sharex=True, sharey=True)\n",
    "\n",
    "for (group_name, group_df), ax in zip(gb, axs.flat):\n",
    "    ax.scatter(*group_df[['acceleration', 'horsepower']].values.T)\n",
    "    ax.set_title(group_name)\n",
    "    ax.annotate(f'r({group_df.shape[0]}) = ' +\n",
    "        f\"{group_df[['acceleration', 'horsepower']].corr().min().iloc[0]:.2f}\",\n",
    "        xy=(0.525, 0.9,), xycoords='axes fraction')\n",
    "    \n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "na_4NT3L76vO"
   },
   "source": [
    "And offers more control compared with the built-in plotting in pandas, which is much more useful for simple exploration. See [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/visualization.html)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "_Xrr9AfS8Mtj"
   },
   "source": [
    "ax = mpg.plot.scatter('acceleration', 'horsepower', )\n",
    "ax.annotate(f'r({mpg.shape[0]}) = ' +\n",
    "        f\"{mpg[['acceleration', 'horsepower']].corr().min().iloc[0]:.2f}\",\n",
    "        xy=(0.525, 0.9,), xycoords='axes fraction')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BA__xcWyAq4L"
   },
   "source": [
    "OK. So we have our aggregated dataframe, and we know what was each step in making it.\n",
    "\n",
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Aggregate the mean and standard deviation of the dataframe, by `origin` and `cylinders`."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MpyU_1HOApeJ"
   },
   "source": [
    "grouped = mpg.groupby(['origin', 'cylinders']).agg(['mean', 'std']).round(2)\n",
    "grouped.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TTqV2KwWDxU0"
   },
   "source": [
    "The result is a `MultiIndex`ed data frame. Here are the basics. See more [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html).\n",
    "\n",
    "----\n",
    "\n",
    "Simple indexing, returning a column."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "RUnSCT7VBhPu"
   },
   "source": [
    "grouped['mpg']['mean']"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d4G0s3VlA4BL"
   },
   "source": [
    "The indexing with MultiIndex can get very complicated. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2df6QG-3EYKk"
   },
   "source": [
    "grouped.loc[(slice('europe', 'japan'), slice(None)), ('mpg', 'mean')]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WCJdG_8KGhhT"
   },
   "source": [
    "Note that the origin and cylinders columns are now missing from the dataframe, they were turned into row indices (so far we only seen integers). We have shown how we can use them in reindexing operation. But sometimes we would want to return them to the data frame (e.g., if we want to use them on further analysis). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KSuljaPhGjn1"
   },
   "source": [
    " # Note that this returns a new dataframe, we can also use `inplace` argument.\n",
    "grouped.reset_index(level='origin')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Li2WB2goHBJx"
   },
   "source": [
    "Or skip this in the first place."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qRnJVgIiHIZP"
   },
   "source": [
    "mpg.groupby(['origin'], as_index=False).mean()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YrZlg8QzHkEg"
   },
   "source": [
    "### Transform\n",
    "\n",
    "Often we would want the aggregation operation to return a data structure that has the same dimensions as the original. For example, when we want to add summary statistics of each group or subject (e.g., think of an experiment with many trials per participant). \n",
    "\n",
    "----\n",
    "\n",
    "\n",
    "`assign` is a function that returns a new dataframe with an additional column, and can be used for elegant chaining."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oRDn7DcQJ4Cx"
   },
   "source": [
    "mpg.assign(\n",
    "    weight_by_origin=mpg.groupby(\n",
    "        'origin', sort=False)['weight'].transform('mean')).tail()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKrohOakLHmh"
   },
   "source": [
    "Note that we have an `NaN` for some for some of the columns, espcially `horsepower`. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "MEOfcw_lOj4E"
   },
   "source": [
    "mpg.info()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hlhdefs0Oit8"
   },
   "source": [
    "One way of imputation is to fill the missing values with some cetral tendency measure. We can do it with the mean or median, for example.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7lWimZLrQf4F"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Fill in the code below, returning the `horsepower` column where missing values will be filled with the dataset median for the column. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lbuurz3lQNzW"
   },
   "source": [
    "mpg['horsepower'].fillna(mpg['horsepower'].agg('median'))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJg6EvCFQfQr"
   },
   "source": [
    "If we want to fill the missing values in the column using the mean of a specific group, here is one option. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jdIJN9xbRwDk"
   },
   "source": [
    "Note the use of the `values` attribute. This ensures that the assignment is of a NumPy array rather than a view of the data frame. \n",
    "\n",
    "----\n",
    "\n",
    "`describe` is a method to get a quick summary of the different columns. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hcq3tvKwHeSN"
   },
   "source": [
    "mpg['horsepower'] = mpg['horsepower'].fillna(mpg.groupby('cylinders')['horsepower'].transform(\n",
    "    'mean')).values\n",
    "mpg.describe().round(2)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TzAxbr5ufIfl"
   },
   "source": [
    "### I / O\n",
    "\n",
    "So far we either created our dataframes by hand, or imported them from a built-in dataset. However, usually you would be working on files.\n",
    "\n",
    "Now is a good time to tell you that Colab runs on a Linux-based machine. As with any computer, we have folders. \n",
    "\n",
    "We can use the exclamation mark to run commands on the shell (\"Command prompt\") of our current machine. This will be very useful later when we get to installing new libraries, which are not already installed on Colab. \n",
    "\n",
    "Here is the current folder contensts, and the contensts of the `sample_data` folder that colab offers us."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "UnnJ344qfLMt"
   },
   "source": [
    "! dir . # Revelas the files in the current folder"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Mwbvyt1WgiRC"
   },
   "source": [
    "! dir ./sample_data # Revelas the files in the folder below the current"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DBpQpjAQgm2g"
   },
   "source": [
    "We see that Colab offers us two famous datasets \"mnist\" and \"california housing\". The files are split into training and testing datasets, so we can easily train a machine learning model to the training set and test on the test set. \n",
    "\n",
    "We will use these files to demonstrate how we read and write data to and from files.\n",
    "\n",
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Fill in the code below to load the california datasets. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xwrvEe9fhHhQ"
   },
   "source": [
    "train = pd.read_csv('sample_data/california_housing_train.csv')\n",
    "test = pd.read_csv('sample_data/california_housing_test.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5YCg98_miIab"
   },
   "source": [
    "Here we combine the two files, by concatenating the two dataframes. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "bYZ9V9TNiQjD"
   },
   "source": [
    "df = pd.concat(\n",
    "    [train, test]\n",
    ")\n",
    "\n",
    "## Another option would be \n",
    "# df = train.append(test)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "929K_bqMjgxX"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Fill in the code below to save the new dataframe into a CSV file named 'california_combined', place it in the same directory as the original files. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SF-h0cu2joA1"
   },
   "source": [
    "df.to_csv('sample_data/california_combined.csv', index=False)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KdSK7rp1jszp"
   },
   "source": [
    "Now let's see if we saved it correctly. \n",
    "\n",
    "In case you are not using Colab, Jupyter Notebook or a similar tool, you might want to use the some module to view files and folders on disk from within Python. One out of many options here would be:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zbjQsrIIkTK6"
   },
   "source": [
    "import glob # Elegantly filter the list of files in the folder\n",
    "glob.glob('sample_data/*.csv')\n",
    "# glob.glob('sample_data/[cali]*.csv')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JjOLrxH5loOU"
   },
   "source": [
    "Pandas can work with many [other formats](https://pandas.pydata.org/pandas-docs/stable/user_guide/io.html) except CSV, like Excel, SPSS, Stata, and more. The process is pretty much the same as we did here.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7fQ8M_oCSM1J"
   },
   "source": [
    "### Misc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rC49EOZbSccw"
   },
   "source": [
    "Often times we want to view the individual values in a specific column or the whole dataframe. `unique` and `value_counts` are useful here. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Lqfx6-NpWb5g"
   },
   "source": [
    "print(mpg['name'].value_counts(), \n",
    "      mpg['name'].unique(), sep='\\n\\n')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qh40QA84WxWq"
   },
   "source": [
    "Note that to do the same we will need to use the `apply` method, that iterates over the columns (or rows). "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SPuG8XprXICG"
   },
   "source": [
    "print(mpg[['cylinders', 'origin']].apply(pd.Series.unique))\n",
    "\n",
    "## A more common form would be \n",
    "mpg[['cylinders', 'origin']].apply(lambda s: s.unique())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rxb4xOGHz0Dk"
   },
   "source": [
    "\n",
    "\n",
    "#### query\n",
    "\n",
    "\n",
    "`query` is a row-selection method of pandas dataframes that can be very elegant, espcially after long chaining operations. \n",
    "\n",
    "Say we want to take all models weighting more than 2000 which were in made USA. Than we take only the summary of the groups who make less than 25 miles per gallon. \n",
    "\n",
    "Consider the two following options:"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "wsuDk6ZolWeO"
   },
   "source": [
    "# This is verbose and error prone\n",
    "# We need to groupby etc. inside loc to get the same dimensions\n",
    "mpg.loc[(\n",
    "    mpg['weight'] > 2000) & (mpg['origin'] == 'usa')].groupby(\n",
    "        'cylinders').median().loc[mpg.loc[(mpg['weight'] > 2000)\n",
    "         & (mpg['origin'] == 'usa')].groupby('cylinders').median()['mpg'] < 25]\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "iD_4YQgUpLqY"
   },
   "source": [
    "# This is succint, readable and easy to debug\n",
    "mpg.query('weight > 2000 & origin == \"usa\"'\n",
    "    ).groupby('cylinders').median().query('mpg < 25')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q4z07BEPxHOM"
   },
   "source": [
    "#### where and mask\n",
    "\n",
    "`where` is another method, which returns a copy of the dataframe, setting to `NaN` every row or cell that is not True according to the filter expression. This is useful if you want to filter rows or columns but get an object which has the same dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "fTOIOxFTxXWe"
   },
   "source": [
    "mpg.where(mpg['name'] == 'buick skylark 320')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cKRN3vCfzY1a"
   },
   "source": [
    "The inverse of `where` is mask."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "69l_DqQJzdP6"
   },
   "source": [
    "mpg.mask(mpg['name'] == 'buick skylark 320')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "agA6CeV8lZIa"
   },
   "source": [
    "### Reshaping\n",
    "\n",
    "Reshaping is the act of changing the structure of a dataframe, like turning rows into columns and vice versa (e.g., \"Pivot table\"). \n",
    "\n",
    "As with any task, Pandas offers a variety of reshaping options. Here are the basics."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "P_EaKvQJnAxT"
   },
   "source": [
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SqUqyrLInEp3"
   },
   "source": [
    "pivotted = pd.pivot_table(tips, \n",
    "               values=['total_bill', 'tip'], index=['smoker', 'time'], \n",
    "               columns='size',\n",
    "               aggfunc='sum') # Can also be median, your own function, etc.\n",
    "\n",
    "pivotted"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SdrHyGzsql0n"
   },
   "source": [
    "Reshaping wide dataframe to long can be achieved usign stack."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pUYjMDH7qu03"
   },
   "source": [
    "tips.groupby(['smoker']).mean().stack().reset_index().rename(\n",
    "    columns={'level_1': 'Variable', 0: 'Mean Value'}\n",
    ")"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WbgD3jPZp1ix"
   },
   "source": [
    "Crosstabbing is another common operation. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "rBi55bFhp5cF"
   },
   "source": [
    "pd.crosstab(tips['smoker'], tips['time'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qDGNzX4gla1u"
   },
   "source": [
    "That's it for pandas, we merely scratched the surface. We didn't cover some very powerful features like windowed operations (e.g., cumulative\\rolling sum), time series and categorical data. To continue on your own, head over [here](https://pandas.pydata.org/pandas-docs/stable/user_guide/index.html). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p3pxYZF-rxf-"
   },
   "source": [
    "## Seaborn\n",
    "\n",
    "Seaborn is a visualiztion library (like Matplotlib), but is built on top of Matplotlib. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "trgCuB1m83eQ"
   },
   "source": [
    "import seaborn as sns\n",
    "tips = sns.load_dataset('tips')\n",
    "tips.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZGa-HHqx9JPl"
   },
   "source": [
    "Seaborn offers many types of plots. Most can be drawn straight onto your subplots object and using variables taken straight from a data frame. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DHm49XrI9Iwx"
   },
   "source": [
    "fig, axs = plt.subplots(1, 2)\n",
    "\n",
    "ax1 = sns.scatterplot(\n",
    "    data=tips, x='total_bill', y='tip', hue='smoker', \n",
    "    ax=axs[0],\n",
    "    legend=None,\n",
    ")\n",
    "\n",
    "ax1.set(xlabel='Total Bill ($)', ylabel='Tip ($)')\n",
    "\n",
    "\n",
    "\n",
    "ax2 = sns.pointplot(\n",
    "    data=tips, x='day', y='tip', hue='smoker',\n",
    "    ax=axs[1],\n",
    "    join=False, err_style=\"bars\", dodge=0.3\n",
    ")\n",
    "\n",
    "ax2.set(xlabel='Day', ylabel='Tip ($)')\n",
    "\n",
    "fig.tight_layout()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r18Ke3c6Ack2"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Generate a figure with subplots in a 3X1 array.\n",
    "Using the `tips` dataset, draw the following from topmost to bottommost.\n",
    "\n",
    "* A horizontal boxplot (`sns.boxplot`) of the number of guests in a party (`size`) on each day. Seperate the boxes into different hues based on the sex of the person paying the waiter.  \n",
    "* A histogram (`sns.histplot`) of the relative size of `tip` to `total_bill`, with the color of the hisograms based on the whether there is a `smoker` in the party. \n",
    "* A line (`sns.lineplot`) showing the trend in `total_bill`' across the values of `size`. Set the style of the lines to whether there is a smoker in the party. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "4vDea8YGBHl8"
   },
   "source": [
    "fig, axs = plt.subplots(3, 1, figsize=(4, 6))\n",
    "\n",
    "sns.boxplot(ax=axs.flat[0], data=tips, x='day', y='total_bill', hue='smoker')\n",
    "axs.flat[0].legend().remove()\n",
    "\n",
    "sns.histplot(ax=axs.flat[1], data=tips, x=tips['tip'] / tips['total_bill'], \n",
    "             hue='smoker', alpha=0.5)\n",
    "\n",
    "sns.lineplot(data=tips, ax=axs.flat[2], x='size', y='tip', \n",
    "             style='smoker')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ToLX4V9AH7Yf"
   },
   "source": [
    "The plots we used so far are axes-level plots. They can either accept an `ax` argument or return a new ax if not given one be to plotted on. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-XEP2KXDJG_Q"
   },
   "source": [
    "df = sns.load_dataset(\"penguins\")\n",
    "df.head()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "uEkTveHGJJkT"
   },
   "source": [
    "ax = sns.kdeplot(data=df, x='bill_length_mm', y='body_mass_g', \n",
    "                 hue=\"sex\", fill=True, alpha=0.5)\n",
    "type(ax)\n",
    "\n",
    "# If we want a handle to the Figure object that Seaborn did not give us\n",
    "fig = plt.gcf()\n",
    "fig.set_facecolor('grey')\n",
    "\n",
    "ax == fig.axes[0]"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5Szuv8_IJGTb"
   },
   "source": [
    "\n",
    "\n",
    "Seaborn also can generate figure level plots. \n",
    "\n",
    "They do not accept an `ax` argument, and are drawn into their own object, usually one that inherits properties from the Matplotlib Figure class."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KjJT8GC_I0fK"
   },
   "source": [
    "pairplot_grid = sns.pairplot(df, hue=\"species\")\n",
    "print(pairplot_grid.axes) # Just like a Figure"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9hucL6guf_uw"
   },
   "source": [
    "df.columns"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kZS7RKoogMem"
   },
   "source": [
    "Another illustrative multi-grid plot. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "zW2Y5Fthftly"
   },
   "source": [
    "marginal_hist = sns.jointplot(data=df, x='bill_length_mm',\n",
    "                              y='bill_depth_mm',\n",
    "                              hue=\"species\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "3oGK38I9geJa"
   },
   "source": [
    "sns.heatmap(df.corr(), annot=True)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FiD3JRtbfQts"
   },
   "source": [
    "To sum up, most multi grid plots are very useful for exploration of data. \n",
    "\n",
    "One last thing to learn about Seaborn is it's `FacetGrid` object. While offering slightly less control than directly using subplots and looping over groups from a GroupBy operation, it can produce nice graphs quickly. See more [here](https://seaborn.pydata.org/tutorial/axis_grids.html#grid-tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mIuoxmwUheXi"
   },
   "source": [
    "from matplotlib import pyplot as plt\n",
    "plt.style.use('ggplot') # Set a nice scheme\n",
    "\n",
    "g = sns.FacetGrid(tips, col=\"sex\", row=\"smoker\", hue='day')\n",
    "g.map(sns.scatterplot, \"total_bill\", \"tip\", alpha=.7)\n",
    "g.add_legend()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1QTH2ljC5KIs"
   },
   "source": [
    "# Statistical analyses using Python\n",
    "\n",
    "There are several libraries in Python that make common statistical tests accessible. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kUC_uLVpiKid"
   },
   "source": [
    "## Statsmodels\n",
    "\n",
    "Statsmodels is one of the major statistics libraries in Python (there is a slightly less rich statistics module under `scipy`, see [here](https://docs.scipy.org/doc/scipy/reference/reference/stats.html)). We will touch very briefly on two common analyses using statsmodels.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1ZxPHlSBjUJV"
   },
   "source": [
    "#### Independent Samples t-test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKIE-G_An94w"
   },
   "source": [
    "Here we will test a simple hypothesis about the penguins dataset. As you can see, the `bill_length_mm` attribute is different between the three species. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "DRlipN4hoRaM"
   },
   "source": [
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "sns.jointplot(data=penguins, x='bill_length_mm',\n",
    "                              y='bill_depth_mm',\n",
    "                              hue=\"species\")\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b-8P0V_Pjo9r"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Statsmodels t-test function receives two arrays x1 and x2, and some other argumensts. \n",
    "\n",
    "Use the Penguin data set to test the hypothesis that the bill length of the `Adelie` species is smaller than that of the `Chinstrap` group.\n",
    "\n",
    "Sample 8 observations from each group. \n",
    "\n",
    "Use a p-value of 0.001.\n",
    "\n",
    "Print out the results using `str.format` method or an f-string."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "yZkx6vjLjd8d"
   },
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from statsmodels.stats.weightstats import ttest_ind\n",
    "\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "\n",
    "\n",
    "penguins = sns.load_dataset('penguins')\n",
    "\n",
    "n = 8\n",
    "dependent_var = 'bill_length_mm'\n",
    "\n",
    "## There are three unique species. Remove the one that we don't need. \n",
    "# penguins['species'].unique()\n",
    "\n",
    "adelie_data = penguins.query('species == \"Adelie\"').sample(\n",
    "    n=n, random_state=42)[dependent_var]\n",
    "chinstrap_data = penguins.query('species == \"Chinstrap\"').sample(\n",
    "    n=n, random_state=42)[dependent_var]\n",
    "\n",
    "t, p_value, dof = ttest_ind(adelie_data, chinstrap_data, \n",
    "                           alternative='smaller')\n",
    "\n",
    "print(f't({dof}) = {t:.2f}, p-value ' + \n",
    "      (f'= {p_value:.3f}' if p_value >= 0.001  else '< .001')\n",
    "      )\n",
    "p_value < .001"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MTIBI6sFpMp3"
   },
   "source": [
    "#### Linear regression\n",
    "\n",
    "Linear regression is another common statistical analysis. Let's use it to predict the bill depth using flipper length and species (as a dummy variable). Then we will see how our model generalizes in prediction. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "OYX40WF1prPy"
   },
   "source": [
    "train, test = train_test_split(penguins.dropna(), test_size=0.2, random_state=10)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "POz_Kt2orYeq"
   },
   "source": [
    "Let's first look at our data. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "QX1b1unIqDlU"
   },
   "source": [
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "for group_name, group_df, ax in zip(['Train', 'Test'], [train, test], axs.flat):\n",
    "    ax2 = sns.scatterplot(data=group_df, x='body_mass_g',\n",
    "                              y='bill_length_mm',\n",
    "                              hue=\"sex\", ax=ax)\n",
    "    ax.set_title(group_name)\n",
    "axs[0].legend().remove()\n",
    "axs[1].legend(bbox_to_anchor=(1.1, 1.05), title='sex')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "5CulhN13rGfZ"
   },
   "source": [
    "reg = ols('bill_length_mm ~ C(sex) * body_mass_g', data=train.dropna()).fit() \n",
    "print(reg.summary())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8opnyRlcuYQY"
   },
   "source": [
    "We see that only the `body_mass_g` variable is a statistically significant predictor for `bill_length_mm`, while `sex` or their interaction isn't significant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5hbfDU62vQ8x"
   },
   "source": [
    "Now let's use our regression model to compare the fit for the training and testing samples. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ios8sn7ovbHi"
   },
   "source": [
    "train['predicted_values'] = reg.predict()\n",
    "test['predicted_values'] = reg.predict(test)\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, sharex=True, sharey=True)\n",
    "for group_name, group_df, ax in zip(\n",
    "    ['Train', 'Test'], [train, test], axs.flat):\n",
    "\n",
    "    sns.scatterplot(data=group_df, x='bill_length_mm',\n",
    "                              y='predicted_values', alpha=1,\n",
    "                              hue=\"sex\", ax=ax)\n",
    "    \n",
    "    y_min, y_max = group_df['predicted_values'].describe()[['min', 'max']].values\n",
    "\n",
    "    ax.set(\n",
    "           xlabel='Actual Bill Length',\n",
    "           ylabel='Predicted Bill Length')\n",
    "    \n",
    "    x_ref = y_ref = np.linspace(y_min, y_max, 100)\n",
    "    ax.plot(x_ref, y_ref, color='black', linewidth=1)\n",
    "    \n",
    "    r_square = (\n",
    "        group_df[['bill_length_mm', 'predicted_values']].corr().pow(2).min().iloc[0])\n",
    "    ax.set_title(f'{group_name} - $r^{2}$ = {r_square:.2f}')\n",
    "\n",
    "axs[0].legend().remove()\n",
    "axs[1].legend(bbox_to_anchor=(1.1, 1.05), title='sex')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3mwTGsqe3-z4"
   },
   "source": [
    "## Installing libraries\n",
    "\n",
    "To install packages in Python need to use a package manager. Luckily, Colab comes with `pip`, the package installer for Python. A package manager keeps track of what packages you have on your current environment, and installs/updates packages accordingly when you want to install a new package. \n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "YrWNMwIV4h85"
   },
   "source": [
    "# To get a list of the installed libraries\n",
    "! pip list"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VAqhYG764qvY"
   },
   "source": [
    "!pip install pingouin"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Z6GRiwB53jP"
   },
   "source": [
    "\n",
    "### Pingouin\n",
    "\n",
    "We just installed [Pingouin](https://pingouin-stats.org/), a Pandas based library useful for many common statistical tests. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YJL-BcRS58S-"
   },
   "source": [
    "#### $\\color{dodgerblue}{\\text{Exercise}}$\n",
    "\n",
    "Import `Pingouin` and load the `attention` dataset from `Seaborn`. \n",
    "conduct a repeated-measures ANOVa using Pingouin, with the following parameters:\n",
    "\n",
    "* `subject` is the participant ID.\n",
    "* `attention` is the between group factor.\n",
    "* `solutions` is the within group (repeated) factor. \n",
    "* `score` is the dependent variable. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2fkELAOX5OMy"
   },
   "source": [
    "import pingouin as pg\n",
    "import seaborn as sns\n",
    "df = sns.load_dataset('attention')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1n5S2-f47Xmq"
   },
   "source": [
    "pg.mixed_anova(\n",
    "    data=df, \n",
    "    dv='score', \n",
    "    between='attention', \n",
    "    within='solutions',\n",
    "    subject='subject',\n",
    "    effsize=\"np2\" # Partial eta-square effect size\n",
    "    )\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Oxb9bCI7h2pG"
   },
   "source": [
    "\n",
    "### robusta\n",
    "\n",
    "**[robusta](https://eitanhemed.github.io/robusta/_build/html/index.html)** is a statistical hypothesis testing in Python that i am currently devloping, it is based on an interface between R and Python. Here is a [demo](https://colab.research.google.com/drive/1jmwYpEGcpFr4CF6ZA5HMiQ2LcHbZqzO_?usp=sharing) of the current state. \n",
    "If we have time we can install it later, when we will be learning about local installations of Python. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AIcnjeXm53m9"
   },
   "source": [
    "# Local Python installation\n",
    "\n",
    "If you want to install Python locally, there are many ways you can go about it. Today we will look at one that would fit most academic researchers that intend on using existing Python tools for data analysis. \n",
    "\n",
    "We will install JupyterLab. JupyterLab is a notebook interface for working with Python (and some other languages, such as R and Julia). \n",
    "\n",
    "* [Download](https://github.com/jupyterlab/jupyterlab-desktop#download) and install JupyterLab App.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "-SD-W-FK6Qps"
   },
   "source": [],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HPiZU254pU7G"
   },
   "source": [
    "# Using your own Google Drive with Colab\n",
    "\n",
    "Sometimes we want to use Google Drive to load or save files, instead of using Python locally. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S_Xy5kcLrJ3Y"
   },
   "source": [
    "First you need to connect your Google Drive. We need to import the `drive` module. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "531Rxt8xp8gF"
   },
   "source": [
    "# Some additional imports just for the demo.\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# What is actually essential - import the drive module \n",
    "from google.colab import drive"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8SSS6Vlq03BY"
   },
   "source": [
    "Now we need to connect Drive. We are mounting it as a folder in the current machine. \n",
    "\n",
    "\n",
    "The current folder prior to mounting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Uy2Rr8720915"
   },
   "source": [
    "os.listdir('.') # Show the contents of the current directory\n",
    "\n",
    "# Can also just be the following like when uncommented\n",
    "#!ls"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a6qlnPy42gO3"
   },
   "source": [
    "Run this and follow the instructions. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J3ynz7wj0xQZ"
   },
   "source": [
    "drive.mount('./drive') # Mount drive in the current directory"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t9grPAQy174p"
   },
   "source": [
    "The contents of the current folder after mounting."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VfMGlSr0wfCn"
   },
   "source": [
    "os.listdir('.')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u2sQ0WeG1t_F"
   },
   "source": [
    "For the sake of the demo create a new folder in your Google Drive using the following. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "oJVl6Me11FkR"
   },
   "source": [
    "new_folder_in_drive = 'drive/MyDrive/python_workshop/example_directory'\n",
    "\n",
    "# Try to create the directory\n",
    "os.makedirs(new_folder_in_drive,\n",
    "          exist_ok=True) \n",
    "\n",
    "# Change the current directory to be the new folder\n",
    "os.chdir(new_folder_in_drive)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GG7Y38UW1nWW"
   },
   "source": [
    "Now we would plot some data and save it on our new drive folder. "
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nUZsbYEMxnix"
   },
   "source": [
    "# Create some figure\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Generate some random data\n",
    "a = np.random.random((16, 16))\n",
    "\n",
    "# Plot the data\n",
    "ax.imshow(a, cmap='hot', interpolation='nearest')\n",
    "\n",
    "# Save the new plot\n",
    "fig.savefig('random_heatmap.png')\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9ZJ_4sWCwbkv"
   },
   "source": [
    "## To end the session uncomment and run the following line\n",
    "#drive.flush_and_unmount()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xcxrj6SR34Pk"
   },
   "source": [
    "These are the essentials in terms of using your Google Drive on Colab. Here is [additional info](https://colab.research.google.com/notebooks/io.ipynb)"
   ]
  }
 ]
}
